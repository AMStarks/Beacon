"""
News Collection Layer
Implements the first layer of the Beacon architecture - gathering news articles from multiple sources
"""

import asyncio
import httpx
import os
from typing import List, Dict, Any
from datetime import datetime
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET

class NewsArticle:
    """Represents a single news article"""
    def __init__(self, title: str, url: str, source: str, content: str = "", 
                 published_at: datetime = None, category: str = "", 
                 country: str = "US", language: str = "en"):
        self.title = title
        self.url = url
        self.source = source
        self.content = content
        self.published_at = published_at or datetime.now()
        self.category = category
        self.country = country
        self.language = language

class NewsCollector:
    """Collects news articles from multiple sources"""
    
    def __init__(self):
        self.api_keys = {
            'newsapi': os.getenv('NEWSAPI_KEY', 'd69a3b23cad345b898a6ee4d6303c69b'),
            'newsdata': os.getenv('NEWSDATA_KEY', 'pub_83ffea09fe7d4707aa82f3b6c5a0da7c')
        }
        
        # News sources configuration
        self.news_sources = {
            'apis': [
                {'name': 'NewsAPI', 'url': 'https://newsapi.org/v2/top-headlines', 'key': 'newsapi'},
                {'name': 'NewsData', 'url': 'https://newsdata.io/api/1/news', 'key': 'newsdata'}
            ],
            'websites': [
                {'name': 'BBC News', 'url': 'https://www.bbc.com/news'},
                {'name': 'Associated Press', 'url': 'https://apnews.com'},
                {'name': 'NPR', 'url': 'https://www.npr.org'},
                {'name': 'ESPN', 'url': 'https://www.espn.com'}
            ],
            'rss_feeds': [
                {'name': 'BBC RSS', 'url': 'http://feeds.bbci.co.uk/news/rss.xml'},
                {'name': 'AP RSS', 'url': 'https://feeds.apnews.com/rss/apf-topnews'},
                {'name': 'NPR RSS', 'url': 'https://feeds.npr.org/1001/rss.xml'}
            ]
        }
    
    async def collect_articles(self) -> List[NewsArticle]:
        """Main method to collect articles from all sources"""
        print("üîç Starting news collection from all sources...")
        
        all_articles = []
        
        # Collect from APIs
        api_articles = await self._collect_from_apis()
        all_articles.extend(api_articles)
        print(f"üì° Collected {len(api_articles)} articles from APIs")
        
        # Collect from websites
        website_articles = await self._collect_from_websites()
        all_articles.extend(website_articles)
        print(f"üåê Collected {len(website_articles)} articles from websites")
        
        # Collect from RSS feeds
        rss_articles = await self._collect_from_rss()
        all_articles.extend(rss_articles)
        print(f"üì° Collected {len(rss_articles)} articles from RSS feeds")
        
        print(f"‚úÖ Total articles collected: {len(all_articles)}")
        return all_articles
    
    async def _collect_from_apis(self) -> List[NewsArticle]:
        """Collect articles from news APIs"""
        articles = []
        
        async with httpx.AsyncClient() as client:
            # NewsAPI
            try:
                response = await client.get(
                    self.news_sources['apis'][0]['url'],
                    params={
                        'apiKey': self.api_keys['newsapi'],
                        'language': 'en',
                        'pageSize': 50,
                        'country': 'us'
                    }
                )
                if response.status_code == 200:
                    data = response.json()
                    for article in data.get('articles', []):
                        if article.get('title') and article.get('url'):
                            articles.append(NewsArticle(
                                title=article['title'],
                                url=article['url'],
                                source=article.get('source', {}).get('name', 'Unknown'),
                                content=article.get('description', ''),
                                published_at=datetime.fromisoformat(article['publishedAt'].replace('Z', '+00:00')) if article.get('publishedAt') else datetime.now(),
                                category=article.get('category', 'general')
                            ))
            except Exception as e:
                print(f"‚ùå Error collecting from NewsAPI: {e}")
        
        return articles
    
    async def _collect_from_websites(self) -> List[NewsArticle]:
        """Collect articles by crawling news websites"""
        articles = []
        
        async with httpx.AsyncClient() as client:
            for source in self.news_sources['websites']:
                try:
                    print(f"üåê Crawling {source['name']}...")
                    response = await client.get(source['url'])
                    
                    if response.status_code == 200:
                        # Parse HTML content
                        soup = BeautifulSoup(response.text, 'html.parser')
                        
                        # Extract article links and titles
                        links = soup.find_all('a', href=True)
                        for link in links[:10]:  # Limit to 10 articles per source
                            href = link.get('href')
                            title = link.get_text(strip=True)
                            
                            if href and title and len(title) > 10:
                                # Make absolute URL
                                if href.startswith('/'):
                                    href = source['url'] + href
                                elif not href.startswith('http'):
                                    continue
                                
                                articles.append(NewsArticle(
                                    title=title,
                                    url=href,
                                    source=source['name'],
                                    content="",
                                    published_at=datetime.now()
                                ))
                    
                except Exception as e:
                    print(f"‚ùå Error crawling {source['name']}: {e}")
        
        return articles
    
    async def _collect_from_rss(self) -> List[NewsArticle]:
        """Collect articles from RSS feeds"""
        articles = []
        
        async with httpx.AsyncClient() as client:
            for feed in self.news_sources['rss_feeds']:
                try:
                    print(f"üì° Parsing RSS feed: {feed['name']}")
                    response = await client.get(feed['url'])
                    
                    if response.status_code == 200:
                        # Parse RSS XML
                        root = ET.fromstring(response.text)
                        
                        # Find all items
                        items = root.findall('.//item')[:10]  # Limit to 10 articles per feed
                        
                        for item in items:
                            title_elem = item.find('title')
                            link_elem = item.find('link')
                            description_elem = item.find('description')
                            
                            if title_elem is not None and link_elem is not None:
                                title = title_elem.text or ''
                                url = link_elem.text or ''
                                content = description_elem.text if description_elem is not None else ''
                                
                                if title and url:
                                    articles.append(NewsArticle(
                                        title=title,
                                        url=url,
                                        source=feed['name'],
                                        content=content,
                                        published_at=datetime.now()
                                    ))
                        
                except Exception as e:
                    print(f"‚ùå Error parsing RSS feed {feed['name']}: {e}")
        
        return articles
